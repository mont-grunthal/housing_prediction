{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard modules for datasets, plotting, and math used in almost all python ML projects.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Functions required for the model secetion process.\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#The models themselves.\n",
    "#linear\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "#nonlinear\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define any functions we will need\n",
    "\n",
    "#Kfold cross validation method using root mean squared error that can automatically handle data \n",
    "#sets that are log scaled w/o giving artificially low error relative to the non scaled model.\n",
    "\n",
    "#Usefull only when compairing model trained on log tranformed data with model trained without.\n",
    "#No longer used in this project at this time.\n",
    "def Kfold_RMSE(X,y,log_data,n,model):\n",
    "    \n",
    "    regressor = model\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    kf = KFold(n_splits=n)\n",
    "    kf.get_n_splits(X)\n",
    "    \n",
    "    rmse_list = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        train_index = train_index \n",
    "        test_index =test_index \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        regressor.fit(X_train,y_train)\n",
    "        predictions = regressor.predict(X_test)\n",
    "        if log_data == True:\n",
    "            error = mean_squared_error(np.exp(y_test), np.exp(predictions))\n",
    "        else:\n",
    "            error = mean_squared_error(y_test, predictions)\n",
    "        rmse = np.sqrt(error)\n",
    "        rmse_list.append(rmse)\n",
    "    \n",
    "    err = np.array(rmse_list)\n",
    "    mean_err = np.sum(err)/len(err)\n",
    "    std = err.std()\n",
    "    print(f\"mean error is: {mean_err:.2f} with a standard deviation of {std:.2f}\")\n",
    "    return err, mean_err, std  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the preprocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the and test/train datasets and split off the target attribute.\n",
    "train_set = pd.read_csv(\"datasets\\\\processed\\\\train.csv\", index_col=[0])\n",
    "test_set = pd.read_csv(\"datasets\\\\processed\\\\test.csv\", index_col=[0])\n",
    "\n",
    "\n",
    "#split the target off from the training set\n",
    "labels_train = train_set[\"median_house_value\"].copy()\n",
    "train = train_set.drop(\"median_house_value\", axis = 1)\n",
    "\n",
    "\n",
    "#split the target off from the testing set\n",
    "labels_test = test_set[\"median_house_value\"].copy()\n",
    "test = test_set.drop(\"median_house_value\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL SELECTION:\n",
    "Now that the data is processed and ready for use, we need to explore models that we may use for the regression task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kfold cross validation and grid search:\n",
    "We will be evaluating each of the possible models using grid search to test each combination of hyperparameters k-fold times. This is time consuming but is very effective in determining the best model and hyperparameters for your data. We will be evaluating support vector regression, random forest regression, and gaussian process regression as our nonlinear models and linear regression, lasso, and elasticnet as our linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine best hyperparameters for SVR\n",
    "gamma = np.zeros(10)\n",
    "for i in range(1,10):\n",
    "    gamma[i] = 1/i**2\n",
    "gamma=gamma[1:9] \n",
    "\n",
    "param_grid_SVR = [\n",
    "    {\"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"degree\": [1,2,3,4,5], \"gamma\": gamma}]\n",
    "\n",
    "#find best set of hyperparameters and preform cross validation\n",
    "SVR_m = SVR()\n",
    "grid_search_SVR = GridSearchCV(SVR_m,param_grid_SVR,cv = 5,scoring = 'neg_mean_squared_error', return_train_score = True)\n",
    "grid_search_SVR.fit(train, labels_train)\n",
    "\n",
    "\n",
    "#print mean cross validation score for model using best hyper parameters\n",
    "results = grid_search_SVR.cv_results_\n",
    "score_param = []\n",
    "for mean_score, params in zip(results[\"mean_test_score\"], results[\"params\"]):\n",
    "    score_param.append(np.sqrt(-mean_score))\n",
    "score = np.array(score_param)\n",
    "best_score_SVR = score.min()\n",
    "best_param = grid_search_SVR.best_params_\n",
    "\n",
    "print(f\"Best mean kfold cross validation error for SVR was {best_score_SVR} using {best_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine best hyperparameters for Random Forest\n",
    "param_grid_RF = [\n",
    "    {\"n_estimators\": [100, 200, 300, 400], \"max_features\": [13,14,15,16],\n",
    "    \"criterion\": [\"mse\", \"mae\"]}]\n",
    "\n",
    "\n",
    "#find best set of hyperparameters and preform cross validation\n",
    "RF_m = RandomForestRegressor()\n",
    "grid_search_RF = GridSearchCV(RF_m,param_grid_RF,cv = 5,scoring = 'neg_mean_squared_error', return_train_score = True)\n",
    "grid_search_RF.fit(train, labels_train)\n",
    "\n",
    "\n",
    "#print mean cross validation score for model using best hyper parameters\n",
    "results = grid_search_RF.cv_results_\n",
    "score_param = []\n",
    "for mean_score, params in zip(results[\"mean_test_score\"], results[\"params\"]):\n",
    "    score_param.append(np.sqrt(-mean_score))\n",
    "score = np.array(score_param)\n",
    "best_score = score.min()\n",
    "best_param = grid_search_RF.best_params_\n",
    "\n",
    "print(f\"Best mean kfold cross validation error for RF was {best_score_RF} using {best_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine best hyperparameters for Gaussian Process Regression\n",
    "param_grid_GP = [\n",
    "    {\"optimizer\": [\"fmin_l_bfgs_b\"]}]\n",
    "\n",
    "\n",
    "#find best set of hyperparameters and preform cross validation\n",
    "GP_m = GaussianProcessRegressor()\n",
    "grid_search_GP = GridSearchCV(GP_m,param_grid_GP,cv = 5,scoring = 'neg_mean_squared_error', return_train_score = True)\n",
    "grid_search_GP.fit(train, labels_train)\n",
    "\n",
    "\n",
    "#print mean cross validation score for model using best hyper parameters\n",
    "results = grid_search_GP.cv_results_\n",
    "score_param = []\n",
    "for mean_score, params in zip(results[\"mean_test_score\"], results[\"params\"]):\n",
    "    score_param.append(np.sqrt(-mean_score))\n",
    "score = np.array(score_param)\n",
    "best_score = score.min()\n",
    "best_param = grid_search_GP.best_params_\n",
    "\n",
    "print(f\"Best mean kfold cross validation error for RF was {best_score_GP} using {best_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine best hyperparameters for Linear Regression\n",
    "param_grid_Linear = [\n",
    "    {\"fit_intercept\": [True,False]}\n",
    "]\n",
    "\n",
    "#find best set of hyperparameters and preform cross validation\n",
    "Linear_m = LinearRegression()\n",
    "grid_search_linear = GridSearchCV(Linear_m,param_grid_Linear,cv = 5,scoring = 'neg_mean_squared_error', return_train_score = True)\n",
    "grid_search_linear.fit(train, labels_train)\n",
    "\n",
    "\n",
    "#print mean cross validation score for model using best hyper parameters\n",
    "results = grid_search_linear.cv_results_\n",
    "score_param = []\n",
    "for mean_score, params in zip(results[\"mean_test_score\"], results[\"params\"]):\n",
    "    score_param.append(np.sqrt(-mean_score))\n",
    "score = np.array(score_param)\n",
    "best_score_L = score.min()\n",
    "best_param = grid_search_linear.best_params_\n",
    "\n",
    "print(f\"Best mean kfold cross validation error was {best_score_L} using {best_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine best hyperparameters for ElasticNet\n",
    "param_grid_Elastic = [\n",
    "    {\"alpha\": np.linspace(0.01,1,10),\n",
    "    \"l1_ratio\": np.linspace(0.01,1,10)}\n",
    "]\n",
    "\n",
    "#find best set of hyperparameters and preform cross validation\n",
    "Elastic_m = ElasticNet()\n",
    "grid_search_Elastic = GridSearchCV(Elastic_m,param_grid_Elastic,cv = 5,scoring = 'neg_mean_squared_error', return_train_score = True)\n",
    "grid_search_Elastic.fit(train, labels_train)\n",
    "\n",
    "#print mean cross validation score for model using best hyper parameters\n",
    "results = grid_search_Elastic.cv_results_\n",
    "score_param = []\n",
    "for mean_score, params in zip(results[\"mean_test_score\"], results[\"params\"]):\n",
    "    score_param.append(np.sqrt(-mean_score))\n",
    "score = np.array(score_param)\n",
    "best_score_Elastic = score.min()\n",
    "best_param = grid_search_Elastic.best_params_\n",
    "\n",
    "print(f\"Best mean kfold cross validation error was {best_score_Elastic} using {best_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine best hyperparameters for LASSO\n",
    "param_grid_lasso = [\n",
    "    {\"alpha\": np.linspace(0.01,1,10)},\n",
    "    {\"fit_intercept\": [True,False]}\n",
    "]\n",
    "\n",
    "#find best set of hyperparameters and preform cross validation\n",
    "Lasso_m = Lasso()\n",
    "grid_search_lasso = GridSearchCV(Lasso_m,param_grid_lasso,cv = 5,scoring = 'neg_mean_squared_error', return_train_score = True)\n",
    "grid_search_lasso.fit(train, labels_train)\n",
    "\n",
    "#print mean cross validation score for model using best hyper parameters\n",
    "results = grid_search_lasso.cv_results_\n",
    "score_param = []\n",
    "for mean_score, params in zip(results[\"mean_test_score\"], results[\"params\"]):\n",
    "    score_param.append(np.sqrt(-mean_score))\n",
    "score = np.array(score_param)\n",
    "best_score = score.min()\n",
    "best_param_lasso = grid_search_lasso.best_params_\n",
    "\n",
    "print(f\"Best mean kfold cross validation error was {best_score_lasso} using {best_param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization error using best hyperparameters\n",
    "Durring the cross validation step, it became clear that linear models do not preform well on this problem. As a result, we will only be considering random forest regression, support vector regression, and gaussian process regression for our final model. It seems that all of the linear models preformed poorly. This is not surprising considering the complex nature of the problem. What may be surprising is that basic linear outpreformed lasso and even elasticnet. This could be explained by the fact that our data is both low in ojects and features. Since both lasso and elasticnet preform feature reduction, they might be prefrorming poorly since there is so little data that any reduction constitutes a notable redution in information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final model evaluation\n",
    "\n",
    "\n",
    "#Random Forest model model generalization error\n",
    "predictions = grid_search_RF.best_estimator_.predict(test)\n",
    "error = mean_squared_error(np.exp(labels_test), np.exp(predictions))\n",
    "rmse_RF = np.sqrt(error)\n",
    "print(f\"Random Forest RMSE generaliziation error: {np.sqrt(error):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
