{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import requirments\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "#import custom transformer class\n",
    "from custom_transformer import CombinedAttributesAdder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define contants\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset into variable\n",
    "def load_housing_data(housing_path = HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "#Niave method to split the dataset into testing and training\n",
    "def split_train_test(data, ratio):\n",
    "    #shuffled list of indices for each object in dataset\n",
    "    np.random.seed(42)\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    #Calculate how many objects should be in the test set\n",
    "    test_size = int(len(data)*ratio)\n",
    "    #grab every element with index less than test size\n",
    "    test_indices = shuffled_indices[:test_size]\n",
    "    #grab every element with index greater than test size\n",
    "    train_indices = shuffled_indices[test_size:]\n",
    "    return data.iloc[train_indices],data.iloc[test_indices]\n",
    "\n",
    "#Kfold cross validation method using root mean squared error that can automatically handle data \n",
    "#sets that are log scaled w/o giving artificially low error relative to the non scaled model.\n",
    "def Kfold_RMSE(X,y,log_data,n,model):\n",
    "    \n",
    "    regressor = model()\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    kf = KFold(n_splits=n)\n",
    "    kf.get_n_splits(X)\n",
    "    \n",
    "    rmse_list = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        train_index = train_index \n",
    "        test_index =test_index \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        regressor.fit(X_train,y_train)\n",
    "        predictions = regressor.predict(X_test)\n",
    "        if log_data == True:\n",
    "            error = mean_squared_error(np.exp(y_test), np.exp(predictions))\n",
    "        else:\n",
    "            error = mean_squared_error(y_test, predictions)\n",
    "        rmse = np.sqrt(error)\n",
    "        rmse_list.append(rmse)\n",
    "    \n",
    "    err = np.array(rmse_list)\n",
    "    mean_err = np.sum(err)/len(err)\n",
    "    std = err.std()\n",
    "    print(f\"mean error is: {mean_err:.2f} with a standard deviation of {std:.2f}\")\n",
    "    return err, mean_err, std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather and load california housing dataset\n",
    "housing = load_housing_data(housing_path = HOUSING_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print summary of housing dataset\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print attribute information\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count missing values in each column\n",
    "print(housing.isna().sum())\n",
    "print(f\"Only {(207/20640): .2%} of toatal_bedrooms is missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get descriptive stats\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hist the numerical attrs\n",
    "housing.hist(bins = 50, figsize = (20,15));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Split into testing and training using stratiefied sampling on the income attribute\n",
    "\n",
    "#create a discrete version of median_income for the strata\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                              bins = [0.0,1.5,3.0,4.5,6,np.inf],\n",
    "                              labels = [1,2,3,4,5])\n",
    "#plot new attr\n",
    "housing[\"income_cat\"].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot geospatial data\n",
    "housing.plot(kind = \"scatter\", x = \"longitude\", y = \"latitude\", alpha = 0.4,\n",
    "            s = housing[\"population\"]/100, label = \"population\", figsize = (10,7),\n",
    "            c = \"median_house_value\", cmap = plt.get_cmap(\"jet\"), colorbar = True)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore how the attributes are correlated to the target attribute\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot house prices vs median income\n",
    "housing.plot(kind = \"scatter\", x= \"median_income\", y = \"median_house_value\", alpha  = 0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Explore diff attr combinations \n",
    "housing[\"beds_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "#Explore how the attributes are correlated to the target attribute\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split The Data Into Test And Train using stratified sampling\n",
    "\n",
    "\n",
    "#create split object\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#take a representative sample of the pop for both test and train.\n",
    "for train_index, test_index in split.split(housing,housing[\"income_cat\"]):\n",
    "    train_set = housing.iloc[train_index]\n",
    "    test_set = housing.iloc[test_index]\n",
    "    \n",
    "#remove the income catagory attribute from test and training sets\n",
    "for set_ in (train_set, test_set):\n",
    "    set_.drop(\"income_cat\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first split the target off from the other attributes in training set\n",
    "housing_training = train_set.drop(\"median_house_value\", axis = 1)\n",
    "labels_train = train_set[\"median_house_value\"].copy()\n",
    "\n",
    "\n",
    "#first split the target off from the other attributes in test set\n",
    "housing_test = test_set.drop(\"median_house_value\", axis = 1)\n",
    "labels_test = test_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the missing data in beadrooms.\n",
    "\n",
    "\n",
    "#initialize imputer object\n",
    "imputer = SimpleImputer(strategy = \"median\")\n",
    "#drop catagorical attrs from train/test\n",
    "housing_num = housing_training.drop(\"ocean_proximity\", axis = 1)\n",
    "housing_num_test = housing_test.drop(\"ocean_proximity\", axis = 1)\n",
    "#fit the imputer on the train set and transorm train/test\n",
    "imputer.fit(housing_num)\n",
    "X_train = imputer.transform(housing_num)\n",
    "X_test = imputer.transform(housing_num_test)\n",
    "#convert training set back to dataframe\n",
    "housing_tr = pd.DataFrame(X_train,columns = housing_num.columns,\n",
    "                         index = housing_num.index)\n",
    "\n",
    "#convert test set back to dataframe\n",
    "housing_tr_test = pd.DataFrame(X_test,columns = housing_num_test.columns,\n",
    "                         index = housing_num_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test now contain all of the numerical attrs with no missing data\n",
    "#Housing_tr excludes Ocean Proximity, which is a string catagorical value\n",
    "#Best to convert this to a numerical type for the algorithms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#isolate catagorical attributes\n",
    "housing_cat = housing_training[[\"ocean_proximity\"]]\n",
    "housing_cat_test = test_set[[\"ocean_proximity\"]]\n",
    "\n",
    "\n",
    "#apply one hot encoding to the catagorical attributes\n",
    "housing_cat_1hot = pd.get_dummies(housing_cat, prefix=[\"ocean_proximity\"], columns = [\"ocean_proximity\"], drop_first=True)\n",
    "housing_cat_1hot_test = pd.get_dummies(housing_cat_test, prefix=[\"ocean_proximity\"], columns = [\"ocean_proximity\"], drop_first=True)\n",
    "\n",
    "#add the encoded attributes back to the training set\n",
    "attrs1 = [housing_tr, housing_cat_1hot]\n",
    "train = pd.concat(attrs1, axis = 1)\n",
    "\n",
    "#add the encoded cat attributes and target back to the testing set\n",
    "attrs2 = [housing_tr_test, housing_cat_1hot_test]\n",
    "test = pd.concat(attrs2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use custom class to add some engineered attributes\n",
    "#These are rooms per household, pop per household, and bedrooms per household\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#initialize our attribute adder.\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=True)\n",
    "\n",
    "\n",
    "#transform training set.\n",
    "train_extra_attribs = attr_adder.transform(train.values,test)\n",
    "\n",
    "#convert training set back to dataframe\n",
    "train = pd.DataFrame(\n",
    "    train_extra_attribs,\n",
    "    columns=list(train.columns)+[\"rooms_per_household\", \"population_per_household\",\"bedrooms_per_room\"],\n",
    "    index=train.index)\n",
    "\n",
    "\n",
    "#transform test set\n",
    "test_extra_attribs = attr_adder.transform(test.values,test)\n",
    "\n",
    "#convert test set back to dataframe\n",
    "test = pd.DataFrame(\n",
    "    test_extra_attribs,\n",
    "    columns=list(test.columns)+[\"rooms_per_household\", \"population_per_household\",\"bedrooms_per_room\"],\n",
    "    index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove outliers from the train set and its labels\n",
    "\n",
    "#check for which points the z score is above a threshhold and remove them\n",
    "z = np.abs(stats.zscore(train))\n",
    "train = train[(z < 5).all(axis=1)]\n",
    "labels_train = labels_train[(z < 5).all(axis=1)]\n",
    "print(len((z < 5).all(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform std_train/std_test and train/test labels to reduce skewness\n",
    "\n",
    "\n",
    "\n",
    "#Remove the one-hot-encoded attrs from the scaling\n",
    "drop_cols = [\"ocean_proximity_INLAND\",\"ocean_proximity_ISLAND\",\"ocean_proximity_NEAR BAY\",\"ocean_proximity_NEAR OCEAN\",\"longitude\",\"latitude\",\"housing_median_age\"]\n",
    "\n",
    "pre_train = train.drop(drop_cols, axis = 1)\n",
    "pre_test = test.drop(drop_cols, axis = 1)\n",
    "\n",
    "\n",
    "#decrease skewness of test/train set with cube root transform\n",
    "pre_train = np.log(pre_train)\n",
    "pre_test = np.log(pre_test)\n",
    "\n",
    "#decrease skewness of target sets with cube root transform\n",
    "labels_train_trans = np.log(labels_train)\n",
    "labels_test_trans = np.log(labels_test)\n",
    "\n",
    "#return datasets back to dataframe\n",
    "pre_train = pd.DataFrame(pre_train,columns = pre_train.columns,\n",
    "                         index = pre_train.index)\n",
    "\n",
    "pre_test = pd.DataFrame(pre_test,columns = pre_test.columns,\n",
    "                         index = pre_test.index)\n",
    "\n",
    "\n",
    "#add the encoded attributes back to the training set\n",
    "attrs1 = [pre_train, train[drop_cols]]\n",
    "pre_train = pd.concat(attrs1, axis = 1)\n",
    "\n",
    "#add the encoded cat attributes and target back to the testing set\n",
    "attrs2 = [pre_test, test[drop_cols]]\n",
    "pre_test = pd.concat(attrs2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the attributes of test/train using standardization\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Remove the one-hot-encoded attrs from the scaling\n",
    "cat_cols = [\"ocean_proximity_INLAND\",\"ocean_proximity_ISLAND\",\"ocean_proximity_NEAR BAY\",\"ocean_proximity_NEAR OCEAN\"]\n",
    "scaled_train = pre_train.drop(cat_cols, axis = 1)\n",
    "scaled_test = pre_test.drop(cat_cols, axis = 1)\n",
    "\n",
    "#fit scaler and transform datasets\n",
    "scaler.fit(scaled_train)\n",
    "X_train = scaler.transform(scaled_train)\n",
    "X_test = scaler.transform(scaled_test)\n",
    "\n",
    "\n",
    "#return datasets back to dataframe\n",
    "X_train = pd.DataFrame(X_train,columns = scaled_train.columns,\n",
    "                         index = scaled_train.index)\n",
    "\n",
    "X_test = pd.DataFrame(X_test,columns = scaled_test.columns,\n",
    "                         index = scaled_test.index)\n",
    "\n",
    "\n",
    "#add the encoded attributes back to the training set\n",
    "attrs1 = [X_train, train[cat_cols]]\n",
    "std_train = pd.concat(attrs1, axis = 1)\n",
    "\n",
    "#add the encoded cat attributes and target back to the testing set\n",
    "attrs2 = [X_test, test[cat_cols]]\n",
    "std_test = pd.concat(attrs2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original distribustions\n",
    "train.hist(bins = 50, figsize = (20,15));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Transformed distributions\n",
    "std_train.hist(bins = 50, figsize = (20,15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of Preprocessing:\n",
    "\n",
    "## We now have 2 groups of data coresponding to use in tree based and non-tree based models.\n",
    "\n",
    "### labels_train and labels_test: \n",
    "These are the target features \n",
    "for all test and train datasets. They have not been \n",
    "preprocessed other that outlier removal.\n",
    "\n",
    "### test and train: \n",
    "These have no missing features, are one-hot-encoded, \n",
    "include our engineered features, and have had outliers removed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### labels_train_trans and labels_test_trans: \n",
    "These are the target features \n",
    "for the preprocessed test and train datasets. They have been transformed to remove skewness and had outliers removed.\n",
    "\n",
    "### std_test and std_train:\n",
    "These have no missing features, are one-hot-encoded,\n",
    "include our engineered features, have been standardized/de-skewed, and have had outliers removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kfold cross validation for Linear Regression, \n",
    "#Support Vector Regression, and Random Forrest.\n",
    "\n",
    "\n",
    "#k fold cross validation for SVR with fully preprocessed datasets\n",
    "scores_SVR_pre, mean_err_pre, std_SVR_pre = Kfold_RMSE(std_train,labels_train_trans, True, 100,SVR)\n",
    "\n",
    "#k fold cross validation for random forest with fully preprocessed datasets\n",
    "scores_RF_pre, mean_err_pre, std_RF_pre = Kfold_RMSE(std_train,labels_train_trans, True, 100, RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression model trained on the fully preprocessed datasets\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(std_train,labels_train_trans)\n",
    "predictions = lin_reg.predict(std_test)\n",
    "error = mean_squared_error(np.exp(labels_test_trans), np.exp(predictions))\n",
    "rmse_LR_pre = np.sqrt(error)\n",
    "print(f\"RMSE generaliziation error: {np.sqrt(error):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression model trained on the unscalled and standardized dataset\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(train,labels_train)\n",
    "predictions = lin_reg.predict(test)\n",
    "error = mean_squared_error(labels_test, predictions)\n",
    "rmse_LR = np.sqrt(error)\n",
    "print(f\"RMSE generaliziation error: {np.sqrt(error):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Regressor model trained on the fully preprocessed datasets\n",
    "SVR_reg = SVR()\n",
    "SVR_reg.fit(std_train,labels_train_trans)\n",
    "predictions = SVR_reg.predict(std_test)\n",
    "\n",
    "error = mean_squared_error(np.exp(labels_test_trans), np.exp(predictions))\n",
    "rmse_SVR_pre = np.sqrt(error)\n",
    "print(f\"SVR RMSE generaliziation error: {np.sqrt(error):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Regressor model trained on the the unscalled and non-standardized dataset\n",
    "SVR_reg = SVR()\n",
    "SVR_reg.fit(train,labels_train)\n",
    "predictions = SVR_reg.predict(test)\n",
    "\n",
    "error = mean_squared_error(labels_test, predictions)\n",
    "rmse_SVR = np.sqrt(error)\n",
    "print(f\"SVR RMSE generaliziation error: {np.sqrt(error):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest model trained on the fully preprocessed datasets\n",
    "RF_reg = RandomForestRegressor()\n",
    "RF_reg.fit(std_train,labels_train_trans)\n",
    "predictions = RF_reg.predict(std_test)\n",
    "\n",
    "error = mean_squared_error(np.exp(labels_test_trans), np.exp(predictions))\n",
    "rmse_RF_pre = np.sqrt(error)\n",
    "print(f\"Random Forest RMSE generaliziation error: {np.sqrt(error):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest model trained on the fully preprocessed datasets\n",
    "RF_reg = RandomForestRegressor()\n",
    "RF_reg.fit(train,labels_train)\n",
    "predictions = RF_reg.predict(test)\n",
    "\n",
    "error = mean_squared_error(labels_test, predictions)\n",
    "rmse_RF = np.sqrt(error)\n",
    "print(f\"Random Forest RMSE generaliziation error: {np.sqrt(error):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
